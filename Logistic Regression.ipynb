{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c117b7-d114-4744-92d0-73dd50c78e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load datasets\n",
    "X = pd.read_csv(\"logisticX.csv\").values  # Convert to numpy array\n",
    "y = pd.read_csv(\"logisticY.csv\").values  # Convert to numpy array\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Compute cost function\n",
    "def compute_cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = sigmoid(X @ theta)\n",
    "    cost = (-1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "    return cost\n",
    "\n",
    "# Gradient descent for logistic regression\n",
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    \n",
    "    for _ in range(num_iters):\n",
    "        gradient = (1/m) * (X.T @ (sigmoid(X @ theta) - y))\n",
    "        theta -= alpha * gradient\n",
    "        cost_history.append(compute_cost(X, y, theta))\n",
    "    \n",
    "    return theta, cost_history\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "m, n = X.shape\n",
    "X = np.c_[np.ones((m, 1)), X]  # Add intercept term\n",
    "\n",
    "# Initialize parameters\n",
    "theta = np.zeros((n + 1, 1))\n",
    "alpha = 0.1  # Learning rate\n",
    "num_iters = 1000  # Iterations for gradient descent\n",
    "\n",
    "# Train logistic regression model\n",
    "final_theta, cost_history = gradient_descent(X, y, theta, alpha, num_iters)\n",
    "\n",
    "# Final cost and learned parameters\n",
    "final_cost = compute_cost(X, y, final_theta)\n",
    "print(\"Final Cost Function Value:\", final_cost)\n",
    "print(\"Learned Parameters:\", final_theta.ravel())\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.scatter(X[:, 1], X[:, 2], c=y.flatten(), cmap='bwr', edgecolors='k')\n",
    "\n",
    "x_vals = np.array([X[:, 1].min(), X[:, 1].max()])\n",
    "y_vals = -(final_theta[0] + final_theta[1] * x_vals) / final_theta[2]\n",
    "plt.plot(x_vals, y_vals, 'k', label='Decision Boundary')\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot cost function vs iteration\n",
    "plt.figure()\n",
    "plt.plot(range(num_iters), cost_history, 'b-', linewidth=2)\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Cost Function Value\")\n",
    "plt.title(\"Cost Function vs Iterations\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Train model with different learning rates\n",
    "num_iters = 100  # Only train for 100 iterations\n",
    "\n",
    "# Train with alpha = 0.1\n",
    "theta_1 = np.zeros((n + 1, 1))\n",
    "theta_1, cost_history_1 = gradient_descent(X, y, theta_1, alpha=0.1, num_iters=num_iters)\n",
    "\n",
    "# Train with alpha = 5\n",
    "theta_2 = np.zeros((n + 1, 1))\n",
    "theta_2, cost_history_2 = gradient_descent(X, y, theta_2, alpha=5, num_iters=num_iters)\n",
    "\n",
    "# Plot cost function vs iteration for both learning rates\n",
    "plt.figure()\n",
    "plt.plot(range(num_iters), cost_history_1, 'b-', label='Alpha = 0.1', linewidth=2)\n",
    "plt.plot(range(num_iters), cost_history_2, 'r-', label='Alpha = 5', linewidth=2)\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Cost Function Value\")\n",
    "plt.title(\"Cost Function vs Iterations for Different Learning Rates\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Predict class labels using final learned parameters\n",
    "y_pred = sigmoid(X @ final_theta) >= 0.5  # Convert probabilities to binary (0 or 1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred)\n",
    "recall = recall_score(y, y_pred)\n",
    "f1 = f1_score(y, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
